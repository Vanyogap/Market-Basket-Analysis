{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apriori Approach to determine Frequent Item Sets"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Student Name : Yogapriya Vankdari\n",
    "Student ID   : 2040973\n",
    "\n",
    "Steps :\n",
    "\n",
    "Step1: Read the csv file\n",
    "Step2: Input Min support and Min Confidence\n",
    "Step3: Find the unique items from the data provided\n",
    "Step4: Prune the unique items to find frequent set\n",
    "Step5: Use the frequent item set obtained in previous step to generated candidates\n",
    "Step6: Find the support count of the candidates\n",
    "Step7: Prune the candidates based on min support value to obtain frequent item sets\n",
    "Step8: Repeatsteps 5,6,7 until no frequent item sets are obtained\n",
    "Step9: Print the output and write it to a file\n",
    "Step10: Generate Association rules based on the frequent item set generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "from timeit import default_timer as timer\n",
    "from itertools import combinations\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "#Final frequent sets declaration\n",
    "n_frequent_dict = {}\n",
    "apriori_dict = {}\n",
    "apriori_frequent_dict = {}\n",
    "\n",
    "#Read from CSV file into DataFrame\n",
    "all_columns = [\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\"]\n",
    "df = pd.read_csv(\"Z:\\Big Data and Data Mining\\Assignments\\CourseWork1\\GroceryStore.csv\",names = all_columns)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#open file to write the intermediate updates\n",
    "output_file = open(\"Z:\\Big Data and Data Mining\\Assignments\\CourseWork1\\Apriori.txt\",'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read minimum support and confidence\n",
    "min_support = input(\"Enter the value of minimum support\\n\")\n",
    "min_confidence = input(\"Enter the value of minimum confidence\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find unique_items in list\n",
    "def unique_items(dataframe):\n",
    "    item_list = dataframe.values.tolist()\n",
    "    flat_item_list = list(chain.from_iterable(item_list))\n",
    "    unique_item_list = list(set(flat_item_list))\n",
    "    unique_item_list = list(filter(None, unique_item_list))\n",
    "    return unique_item_list    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to write list into file and print it on screen\n",
    "def write_list_to_file(message,item_list):\n",
    "    item_list_str = \"\\n\".join(item_list)\n",
    "    output_file.write(message + item_list_str)\n",
    "    print(message + item_list_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#support count of candidates\n",
    "def find_support(n,item_list,data_list):\n",
    "    k = 0\n",
    "    count = 0\n",
    "    support = [0]*len(item_list)\n",
    "    item = [0]*len(item_list)\n",
    "    for i in item_list:\n",
    "        for j in data_list:\n",
    "            data_set = set(j)\n",
    "            if(i.issubset(data_set)):\n",
    "                  count  +=1\n",
    "        support[k] = count\n",
    "        item[k] = i\n",
    "        apriori_dict[i] = count\n",
    "        k+=1\n",
    "        count = 0\n",
    "    return support,item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find candidates\n",
    "def combine(frequent_set,n):\n",
    "    combined_set = set()\n",
    "    for x in frequent_set:\n",
    "        i = set(x)\n",
    "        for y in frequent_set:\n",
    "            j = set(y)\n",
    "            if(len(i.union(j)) == n):\n",
    "                combined_temp = [x.union(y)]\n",
    "                combined_set = combined_set.union(combined_temp)\n",
    "    return combined_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pruning candidates to find frequent item sets\n",
    "def prune_candidates(min_support,support,item):\n",
    "    frequent_itemset = set()\n",
    "    infrequent_itemset = set()\n",
    "    frequent_count = []\n",
    "    for i in range (len(item)):\n",
    "        if(support[i] >= int(min_support)):\n",
    "            frequent_itemset.add(item[i])\n",
    "            frequent_count.append(support[i])\n",
    "        else:\n",
    "            infrequent_itemset.add(item[i])\n",
    "    return frequent_itemset,infrequent_itemset,frequent_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#candidate set generation\n",
    "def candidate_set(combination_set,infrequent_set):\n",
    "    candidates = combination_set.copy()\n",
    "    for i in combination_set:\n",
    "        for j in infrequent_set:\n",
    "            if(j.issubset(i)):\n",
    "                candidates.remove(i)\n",
    "                break\n",
    "    return candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the built frequent sets\n",
    "def store(freq_set,freq_count,n):\n",
    "    frequent_dict ={}\n",
    "    for i,j in zip(freq_set,range(len(freq_count))):   \n",
    "        n_frequent_dict[i] = freq_count[j] \n",
    "        frequent_dict[i] = freq_count[j] \n",
    "    if(len(frequent_dict) >= 1):\n",
    "        item_set_sel = \"{0}_item_sets\".format(n)\n",
    "        apriori_frequent_dict[item_set_sel] = frequent_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apriori Algorithm\n",
    "def apriori(dataframe,n):\n",
    "    \n",
    "    start = timer()\n",
    "    #Find unique items\n",
    "    unique_item_list = unique_items(dataframe)\n",
    "    item_list = [x for x in unique_item_list if pd.isnull(x) == False]\n",
    "    data_list = df.values.tolist() \n",
    "    \n",
    "    #unique frozen set\n",
    "    item_set = set()\n",
    "    for i in item_list:\n",
    "        if i:\n",
    "            item_set.add(frozenset([i]))\n",
    "            \n",
    "    #Find candidates,support and 1 frequent set\n",
    "    \n",
    "    count,item = find_support(1,item_set,data_list)  \n",
    "    frequent_set,infrequent_set,frequent_count = prune_candidates(min_support,count,item)\n",
    "    store(frequent_set,frequent_count,1)\n",
    "        \n",
    "    #     Find 2-n frequent itemsets\n",
    "    #     Step1 : Find possible combinations\n",
    "    #     Step2 : Find candidates\n",
    "    #     Step3 : Find support count\n",
    "    #     Step4 : Prune candidates that donot satisfy min support condition to generate frequent itemsets\n",
    "    #     Step5 : Save the result into a dictionary for easy access\n",
    "    \n",
    "    for i in range(2,n):\n",
    "        combination_set = combine(frequent_set,i)\n",
    "        candidates = candidate_set(combination_set,infrequent_set)\n",
    "        count,item = find_support(i,candidates,data_list)\n",
    "        frequent_set,infrequent_set,frequent_count = prune_candidates(min_support,count,item)\n",
    "        store(frequent_set,frequent_count,i)\n",
    "    \n",
    "    end = timer()\n",
    "    \n",
    "#     Preparing to write output to file and to print\n",
    "    i = 0\n",
    "    frequent_item_str = [0]*len(n_frequent_dict)\n",
    "    for key, value in n_frequent_dict.items():\n",
    "        frequent_item_str[i] = str('{}----->{}'.format(list(key), value))\n",
    "        i = i+1 \n",
    "        \n",
    "    #calculate time taken to execute for bruteforce approach\n",
    "    total_time = end - start\n",
    "    \n",
    "    #Write output to file and print the output\n",
    "    output_file.write(\"1.minimum support :\" + min_support)\n",
    "    print(\"1.minimum support:\"+min_support)\n",
    "    output_file.write(\"\\n\\n2.minimum confidence :\" + min_confidence)\n",
    "    print(\"\\n\\n2.minimum confidence :\" + min_confidence)\n",
    "    output_file.write(\"\\n\\n3.Apriori Approach:\\n\\n\") \n",
    "    print(\"\\n\\n3.Apriori Approach:\") \n",
    "    write_list_to_file(\"\\n\\n3a.Unique item list:\\n\",item_list)\n",
    "    write_list_to_file(\"\\n\\n\\n3b.frequent item sets:\\n\",frequent_item_str)\n",
    "    output_file.write(\"\\n\\n4.Time taken for execution using Apriori approach:\"+str(total_time)) \n",
    "    print(\"\\n\\n4.Time taken for execution using Apriori approach:\" + str(total_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apriori function call\n",
    "apriori(df,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to determine Association Rules\n",
    "def association_rules():\n",
    "    min_confidence_value = int(min_confidence)/100\n",
    "    output_file.write(\"\\n\\n5.Association Rules:\\n\")\n",
    "    n = len(apriori_frequent_dict)\n",
    "    for i in range(n,0,-1):\n",
    "        item_set_sel = \"{0}_item_sets\".format(i)\n",
    "        freq_len = len(apriori_frequent_dict[item_set_sel])\n",
    "        for j in range(0,freq_len):\n",
    "            test_list = []\n",
    "            test_item = {}\n",
    "            test_item = (apriori_frequent_dict[item_set_sel])\n",
    "            test_list = list(apriori_frequent_dict[item_set_sel].keys())\n",
    "            for k in range(1,i):\n",
    "                index = test_list[j]\n",
    "                numerator = apriori_dict[index]  \n",
    "                comb = combinations(test_list[j], k)\n",
    "                for l in comb:\n",
    "                    z = frozenset(l)\n",
    "                    h=\"{0}_item_sets\".format(len(l))\n",
    "                    deno = apriori_dict[z]\n",
    "                    if((numerator/deno)>=min_confidence_value):\n",
    "                        print((str(l)+\"   -->\"+str(set(test_list[j])-set(l)))+\"      :\"+str(round((numerator/deno)*100,2))+\"%\")\n",
    "                        output_file.write((\"\\n\"+str(l)+\"   -->\"+str(set(test_list[j])-set(l)))+\"      :\"+str(round((numerator/deno)*100,2))+\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Association rules function call\n",
    "association_rules()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close output file after writing\n",
    "output_file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
